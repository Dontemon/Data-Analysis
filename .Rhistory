select(-1)
#удаление дубликатов
diamonds = diamonds %>%
filter(!duplicated(.))
replace_outliers_with_mean_exclude = function(column) {
Q1 = quantile(column, 0.25)
Q3 = quantile(column, 0.75)
IQR = Q3 - Q1
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR
# Определение выбросов
outliers = column < lower_bound | column > upper_bound
# Замена выбросов на среднее значение, исключая выбросы
column[outliers] = mean(column, na.rm = TRUE)
return(column)
}
#В данных очень много выбросов
boxplot(diamonds$carat)
diamonds$carat=replace_outliers_with_mean_exclude(diamonds$carat); boxplot(diamonds$carat)
diamonds$carat=replace_outliers_with_mean_exclude(diamonds$carat); boxplot(diamonds$carat)
boxplot(diamonds$depth)
diamonds$depth=replace_outliers_with_mean_exclude(diamonds$depth); boxplot(diamonds$depth)
diamonds$depth=replace_outliers_with_mean_exclude(diamonds$depth); boxplot(diamonds$depth)
diamonds$depth=replace_outliers_with_mean_exclude(diamonds$depth); boxplot(diamonds$depth)
diamonds$depth=replace_outliers_with_mean_exclude(diamonds$depth); boxplot(diamonds$depth)
boxplot(diamonds$table)
diamonds$table=replace_outliers_with_mean_exclude(diamonds$table); boxplot(diamonds$table)
boxplot(diamonds$x)
diamonds$x=replace_outliers_with_mean_exclude(diamonds$x); boxplot(diamonds$x)
boxplot(diamonds$y)
diamonds$y=replace_outliers_with_mean_exclude(diamonds$y); boxplot(diamonds$y)
boxplot(diamonds$z)
diamonds$z=replace_outliers_with_mean_exclude(diamonds$z); boxplot(diamonds$z)
#Вывод информации о типе и структуре данных
str(diamonds); summary(diamonds)
#Сохранение средних и среднекв.отклонений (для нормализации введенных данных в приложении)
means = colMeans(diamonds[, c("carat", "depth", "table", "x", "y", "z")])
sds = apply(diamonds[, c("carat", "depth", "table", "x", "y", "z")], 2, sd)
means_table = data.frame(
carat = means["carat"],
depth = means["depth"],
table = means["table"],
x = means["x"],
y = means["y"],
z = means["z"]
)
write_xlsx(means_table, "means.xlsx")
sds_table = data.frame(
carat = sds["carat"],
depth = sds["depth"],
table = sds["table"],
x = sds["x"],
y = sds["y"],
z = sds["z"]
)
write_xlsx(sds_table, "sds.xlsx")
#Нормализация данных
diamonds$carat = scale(diamonds$carat)
diamonds$depth = scale(diamonds$depth)
diamonds$table = scale(diamonds$table)
diamonds$x = scale(diamonds$x)
diamonds$y = scale(diamonds$y)
diamonds$z = scale(diamonds$z)
#Корреляционный анализ ----
data = diamonds
#преобразование категориальных факторов для проверки корреляции
cut_levels = c("Fair" = 1, "Good" = 2, "Very Good" = 3, "Ideal" = 4, "Premium" = 5)
data$cut = cut_levels[data$cut]
color_levels = c("D" = 7, "E" = 6, "F" = 5, "G" = 4, "H" = 3, "I" = 2, "J" = 1)
data$color = color_levels[data$color]
clarity_levels = c("I1" = 1, "SI2" = 2, "SI1" = 3, "VS2" = 4, "VS1" = 5, "VVS2" = 6, "VVS1" = 7, "IF" = 8)
data$clarity = clarity_levels[data$clarity]
#корреляционная матрица
data = subset(data, select = c("price", "carat", "cut", "color", "clarity", "depth", "table", "x", "y","z"))
cor(data)
#график корреляций
corrplot(cor(data), method = "color", type = "lower")
# Random Forest ----
diamonds = subset(diamonds, select = c("carat", "cut", "color", "clarity", "depth", "table", "price", "x", "y","z"))
#преобразование факторов
diamonds$cut = as.factor(diamonds$cut)
diamonds$color = as.factor(diamonds$color)
diamonds$clarity = as.factor(diamonds$clarity)
set.seed(123)
# Разделение данных на обучающий и тестовый наборы
sample_size = floor(0.7 * nrow(diamonds))
train_indices = sample(seq_len(nrow(diamonds)), size = sample_size)
train = diamonds[train_indices, ]
test = diamonds[-train_indices, ]
# Создание матриц DMatrix для обучающей и тестовой выборок
train[] <- lapply(train, as.numeric) #приводим значения столбцов обучающей выборки к числовому типу
test[] <- lapply(test, as.numeric) #приводим значения столбцов тестовой выборки к числовому типу
dtrain <- xgb.DMatrix(as.matrix(train[, -1]), label = train$price) #создаем матрицу для обучающей выборки, убираем первый столбец и указываем целевую переменную
dtest <- xgb.DMatrix(as.matrix(test[, -1]), label = test$price) #создаем матрицу для тестовой выборки, убираем первый столбец и указываем целевую переменную
# Определение параметров модели XGBoost
params <- list( #создаем список параметров модели
objective = "reg:squarederror", #целевая переменная для регрессии
eta = 0.01, #скорость обучения (learning rate)
max_depth = 6, #максимальная глубина дерева
min_child_weight = 1, #минимальное количество обучающих примеров в листе дерева
subsample = 0.8, #доля обучающих примеров, используемых для обучения каждого дерева
colsample_bytree = 0.8 #доля признаков, используемых для обучения каждого дерева
)
watchlist <- list(train = dtrain, valid = dtest) #список данных для отслеживания обучения
# Обучение модели на обучающей выборке
model <- xgb.train( #обучаем модель
params = params, #передаем параметры модели
data = dtrain, #передаем данные для обучения
nrounds = 5000, #максимальное количество итераций
early_stopping_rounds = 10, #количество итераций без улучшения модели, при которых обучение останавливается
watchlist = watchlist, #передаем список данных для отслеживания обучения
verbose = 0 #уровень вывода информации об обучении модели
)
predictions = predict(model, newdata = dtest)
rmse3 = sqrt(mean((test$price - predictions)^2))
cat("RMSE: ", rmse3, "\n")
new_data = data.frame(carat = 1.26, cut = "Ideal", color = "G",	clarity = "VVS2",	depth=60.7,	table=56,	x=7.05, y=7.03,	z=4.27) #22520 строка Лучший результат 10581.99
new_data$carat = (new_data$carat - means["carat"]) / sds["carat"]
new_data$depth = (new_data$depth - means["depth"]) / sds["depth"]
new_data$table = (new_data$table - means["table"]) / sds["table"]
new_data$x = (new_data$x - means["x"]) / sds["x"]
new_data$y = (new_data$y - means["y"]) / sds["y"]
new_data$z = (new_data$z - means["z"]) / sds["z"]
new_data$cut <- as.numeric(as.factor(new_data$cut))
new_data$color <- as.numeric(as.factor(new_data$color))
new_data$clarity <- as.numeric(as.factor(new_data$clarity))
colnames(new_data) <- colnames(train)[2:10]
dnew <- xgb.DMatrix(as.matrix(new_data))
new_price <- predict(model, dnew)
new_price
tr_model <- lm(price ~ carat + cut + color + clarity + depth + table + x + y + z, data = diamonds)
tr_model
summary(tr_model)
tr_model <- lm(price ~ carat + cut + color + clarity + depth + x + y + z, data = diamonds)
summary(tr_model)
predict(tr_model, new_data)
new_data = data.frame(carat = 1.26, cut = "Ideal", color = "G",	clarity = "VVS2",	depth=60.7,	table=56,	x=7.05, y=7.03,	z=4.27) #22520 строка Лучший результат 10581.99
new_data$carat = (new_data$carat - means["carat"]) / sds["carat"]
new_data$depth = (new_data$depth - means["depth"]) / sds["depth"]
new_data$table = (new_data$table - means["table"]) / sds["table"]
new_data$x = (new_data$x - means["x"]) / sds["x"]
new_data$y = (new_data$y - means["y"]) / sds["y"]
new_data$z = (new_data$z - means["z"]) / sds["z"]
new_data$cut <- as.numeric(as.factor(new_data$cut))
new_data$color <- as.numeric(as.factor(new_data$color))
new_data$clarity <- as.numeric(as.factor(new_data$clarity))
predict(tr_model, new_data)
new_data = data.frame(carat = 1.26, cut = "Ideal", color = "G",	clarity = "VVS2",	depth=60.7,	table=56,	x=7.05, y=7.03,	z=4.27) #22520 строка Лучший результат 10581.99
new_data$carat = (new_data$carat - means["carat"]) / sds["carat"]
new_data$depth = (new_data$depth - means["depth"]) / sds["depth"]
new_data$table = (new_data$table - means["table"]) / sds["table"]
new_data$x = (new_data$x - means["x"]) / sds["x"]
new_data$y = (new_data$y - means["y"]) / sds["y"]
new_data$z = (new_data$z - means["z"]) / sds["z"]
predict(tr_model, new_data)
new_data = data.frame(carat = 1.26, cut = "Ideal", color = "G",	clarity = "VVS2",	depth=60.7,	table=56,	x=7.05, y=7.03,	z=4.27) #22520 строка Лучший результат 10581.99
predict(tr_model, new_data)
# Linear Regression
linear_model <- lm(price ~ ., data = train)
# Summary of the linear regression model
summary(linear_model)
# Predict on the test set
linear_predictions <- predict(linear_model, newdata = test)
# Calculate RMSE
linear_rmse <- sqrt(mean((test$price - linear_predictions)^2))
cat("Linear Regression RMSE: ", linear_rmse, "\n")
# Predict on new data
new_data$cut <- as.numeric(as.factor(new_data$cut))
new_data$color <- as.numeric(as.factor(new_data$color))
new_data$clarity <- as.numeric(as.factor(new_data$clarity))
new_linear_price <- predict(linear_model, newdata = new_data)
new_linear_price
# Linear Regression
linear_model <- lm(price ~ carat + cut + color + clarity + depth + table,, data = train)
# Summary of the linear regression model
summary(linear_model)
# Predict on the test set
linear_predictions <- predict(linear_model, newdata = test)
# Calculate RMSE
linear_rmse <- sqrt(mean((test$price - linear_predictions)^2))
cat("Linear Regression RMSE: ", linear_rmse, "\n")
new_data <- data.frame(
carat = 1,
cut = "Good",
color = "F",
clarity = "SI1",
depth = 65,
table = 70
)
# Predict on new data
new_data$cut <- as.numeric(as.factor(new_data$cut))
new_data$color <- as.numeric(as.factor(new_data$color))
new_data$clarity <- as.numeric(as.factor(new_data$clarity))
new_linear_price <- predict(linear_model, newdata = new_data)
new_linear_price
new_data = data.frame(carat = 1.26, cut = "Ideal", color = "G",	clarity = "VVS2",	depth=60.7,	table=56,	x=7.05, y=7.03,	z=4.27) #22520 строка Лучший результат 10581.99
# Linear Regression
linear_model <- lm(price ~ ., data = train)
# Summary of the linear regression model
summary(linear_model)
# Predict on the test set
linear_predictions <- predict(linear_model, newdata = test)
# Calculate RMSE
linear_rmse <- sqrt(mean((test$price - linear_predictions)^2))
cat("Linear Regression RMSE: ", linear_rmse, "\n")
# Predict on new data
new_data$cut <- as.numeric(as.factor(new_data$cut))
new_data$color <- as.numeric(as.factor(new_data$color))
new_data$clarity <- as.numeric(as.factor(new_data$clarity))
new_linear_price <- predict(linear_model, newdata = new_data)
new_linear_price
install.packages("class")
install.packages("class")
library(class)
# Define the response variable
knn_response <- as.factor(train$price)
# Train the kNN model with k = 5 (you can adjust k as needed)
knn_model <- knn(train = knn_features, test = as.matrix(test[, c("carat", "cut", "color", "clarity", "depth", "table", "x", "y", "z")]), cl = knn_response, k = 5)
# Train the kNN model with k = 5 (you can adjust k as needed)
knn_model <- knn(train, test = as.matrix(test[, c("carat", "cut", "color", "clarity", "depth", "table", "x", "y", "z")]), cl = knn_response, k = 5)
# k-nearest neighbors method ----
knn_features <- train[, c("carat", "cut", "color", "clarity", "depth", "table", "x", "y", "z")]
# Define the response variable
knn_response <- as.factor(train$price)
# Train the kNN model with k = 5 (you can adjust k as needed)
knn_model <- knn(train = knn_features, test = as.matrix(test[, c("carat", "cut", "color", "clarity", "depth", "table", "x", "y", "z")]), cl = knn_response, k = 5)
# Predict on the test set
knn_predictions <- as.numeric(knn_model)
# Calculate RMSE for kNN
knn_rmse <- sqrt(mean((test$price - knn_predictions)^2))
cat("k-Nearest Neighbors RMSE: ", knn_rmse, "\n")
# Predict on new data
new_data$cut <- as.numeric(as.factor(new_data$cut))
new_data$color <- as.numeric(as.factor(new_data$color))
new_data$clarity <- as.numeric(as.factor(new_data$clarity))
new_knn_predictions <- knn(train = knn_features, test = as.matrix(new_data), cl = knn_response, k = 5)
new_knn_predictions
set.seed(123)
# Выбор количества ближайших соседей (k)
k_neighbors <- 5
# Обучение модели k-NN
knn_model <- knn(train = train[, -9], test = test[, -9], cl = train$price, k = k_neighbors)
# Предсказание на тестовом наборе
predictions_knn <- as.numeric(knn_model)
rmse_knn <- sqrt(mean((test$price - predictions_knn)^2))
cat("k-NN RMSE: ", rmse_knn, "\n")
new_price <- predict(knn_model, new_data)
new_price
new_data = data.frame(carat = 0.31, color = "J",	clarity = "SI2",	x=4.34, y=4.35,	z=2.75) #22520 строка Лучший результат 10581.99
# Нормализуем новые данные с использованием средних и стандартных отклонений из таблицы diamonds
new_data$carat = (new_data$carat - means["carat"]) / sds["carat"]
new_data$depth = (new_data$depth - means["depth"]) / sds["depth"]
new_data$table = (new_data$table - means["table"]) / sds["table"]
new_data$x = (new_data$x - means["x"]) / sds["x"]
new_data$y = (new_data$y - means["y"]) / sds["y"]
new_data$z = (new_data$z - means["z"]) / sds["z"]
new_data$color = factor(new_data$color, levels = levels(diamonds$color))
new_data$clarity = factor(new_data$clarity, levels = levels(diamonds$clarity))
new_data$cut = factor(new_data$cut, levels = levels(diamonds$cut))
prediction = predict(rf_model, new_data)
prediction
library(shiny)
library(corrplot)
library(ggplot2)
library(randomForest)
library(xgboost)
library(readr)
library(dplyr)
library(psych)
library(writexl)
library(caret)
library(class)
# Очистка рабочего пространства ----
while (dev.cur() != 1) {
dev.off()
}
rm(list=ls())
cat("\014")
# Загрузка данных ----
#price - price in US dollars (\$326--\$18,823)
#carat (0,2-5,01) - weight of the diamond (0.2--5.01)
#cut - quality of the cut (Fair, Good, Very Good, Premium, Ideal)
#color - diamond color, from J (worst) to D (best)
#clarity - a measurement of how clear the diamond is (I1 (worst), SI2, SI1, VS2, VS1, VVS2, VVS1, IF (best))
#depth - total depth percentage = z / mean(x, y) = 2 * z / (x + y) (43--79)
#table - width of top of diamond relative to widest point (43--95)
#x - length in mm (0--10.74)
#y - width in mm (0--58.9)
#z - depth in mm (0--31.8)
setwd("D:/Data-Analysis/") #установка рабочей директории
diamonds = read.csv("diamonds.csv") #чтение данных из файла
#write_xlsx(diamonds, "diamonds.xlsx")
#Заполнение нулевых значений средними значениями
diamonds = diamonds %>%
mutate(x = ifelse(x == 0, mean(x[x != 0], na.rm = TRUE), x),
y = ifelse(y == 0, mean(y[y != 0], na.rm = TRUE), y),
z = ifelse(z == 0, mean(z[z != 0], na.rm = TRUE), z))
#удаление первого столбца (индексация)
diamonds = diamonds %>%
select(-1)
#удаление дубликатов
diamonds = diamonds %>%
filter(!duplicated(.))
replace_outliers_with_mean_exclude = function(column) {
Q1 = quantile(column, 0.25)
Q3 = quantile(column, 0.75)
IQR = Q3 - Q1
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR
# Определение выбросов
outliers = column < lower_bound | column > upper_bound
# Замена выбросов на среднее значение, исключая выбросы
column[outliers] = mean(column, na.rm = TRUE)
return(column)
}
#В данных очень много выбросов
boxplot(diamonds$carat)
diamonds$carat=replace_outliers_with_mean_exclude(diamonds$carat); boxplot(diamonds$carat)
diamonds$carat=replace_outliers_with_mean_exclude(diamonds$carat); boxplot(diamonds$carat)
boxplot(diamonds$depth)
diamonds$depth=replace_outliers_with_mean_exclude(diamonds$depth); boxplot(diamonds$depth)
diamonds$depth=replace_outliers_with_mean_exclude(diamonds$depth); boxplot(diamonds$depth)
diamonds$depth=replace_outliers_with_mean_exclude(diamonds$depth); boxplot(diamonds$depth)
diamonds$depth=replace_outliers_with_mean_exclude(diamonds$depth); boxplot(diamonds$depth)
boxplot(diamonds$table)
diamonds$table=replace_outliers_with_mean_exclude(diamonds$table); boxplot(diamonds$table)
boxplot(diamonds$x)
diamonds$x=replace_outliers_with_mean_exclude(diamonds$x); boxplot(diamonds$x)
boxplot(diamonds$y)
diamonds$y=replace_outliers_with_mean_exclude(diamonds$y); boxplot(diamonds$y)
boxplot(diamonds$z)
diamonds$z=replace_outliers_with_mean_exclude(diamonds$z); boxplot(diamonds$z)
#Вывод информации о типе и структуре данных
str(diamonds); summary(diamonds)
#Сохранение средних и среднекв.отклонений (для нормализации введенных данных в приложении)
means = colMeans(diamonds[, c("carat", "depth", "table", "x", "y", "z")])
sds = apply(diamonds[, c("carat", "depth", "table", "x", "y", "z")], 2, sd)
means_table = data.frame(
carat = means["carat"],
depth = means["depth"],
table = means["table"],
x = means["x"],
y = means["y"],
z = means["z"]
)
write_xlsx(means_table, "means.xlsx")
sds_table = data.frame(
carat = sds["carat"],
depth = sds["depth"],
table = sds["table"],
x = sds["x"],
y = sds["y"],
z = sds["z"]
)
write_xlsx(sds_table, "sds.xlsx")
#Нормализация данных
diamonds$carat = scale(diamonds$carat)
diamonds$depth = scale(diamonds$depth)
diamonds$table = scale(diamonds$table)
diamonds$x = scale(diamonds$x)
diamonds$y = scale(diamonds$y)
diamonds$z = scale(diamonds$z)
#Корреляционный анализ ----
data = diamonds
#преобразование категориальных факторов для проверки корреляции
cut_levels = c("Fair" = 1, "Good" = 2, "Very Good" = 3, "Ideal" = 4, "Premium" = 5)
data$cut = cut_levels[data$cut]
color_levels = c("D" = 7, "E" = 6, "F" = 5, "G" = 4, "H" = 3, "I" = 2, "J" = 1)
data$color = color_levels[data$color]
clarity_levels = c("I1" = 1, "SI2" = 2, "SI1" = 3, "VS2" = 4, "VS1" = 5, "VVS2" = 6, "VVS1" = 7, "IF" = 8)
data$clarity = clarity_levels[data$clarity]
#корреляционная матрица
data = subset(data, select = c("price", "carat", "cut", "color", "clarity", "depth", "table", "x", "y","z"))
cor(data)
#график корреляций
corrplot(cor(data), method = "color", type = "lower")
# Random Forest ----
diamonds = subset(diamonds, select = c("carat", "cut", "color", "clarity", "depth", "table", "price", "x", "y","z"))
#преобразование факторов
diamonds$cut = as.factor(diamonds$cut)
diamonds$color = as.factor(diamonds$color)
diamonds$clarity = as.factor(diamonds$clarity)
set.seed(123)
# Разделение данных на обучающий и тестовый наборы
sample_size = floor(0.7 * nrow(diamonds))
train_indices = sample(seq_len(nrow(diamonds)), size = sample_size)
train = diamonds[train_indices, ]
test = diamonds[-train_indices, ]
#Исследовательские графики
#График, показывающий распределение цен в зависимости от качества огранки и цвета бриллиантов
ggplot(train)+
geom_boxplot(aes(y=price,x=cut,fill=cut),outlier.size=0.1,notch=T,notchwidth=0.2)+
facet_grid(~color,margins=T)+
theme(axis.text.x = element_text(angle = 90, hjust = 1))+
ylab(label='Price')+
xlab('Cut')+
ggtitle('Price distribution vs Cut for each colour')
#График, показывающий распределение цен в зависимости от веса бриллианта и качества огранки
ggplot(train)+
geom_point(aes(y=price,x=carat,color=cut))+
scale_x_log10()+scale_y_log10()+
facet_grid(~cut)+
ylab('Price')+
xlab('Carat')+
ggtitle('Price distribution vs Carat')
#График, показывающий распределение цен в зависимости от веса бриллианта
ggplot(train)+
geom_point(aes(y=price,x=carat,color=cut))+
ylab('Price')+
xlab('Carat')+
ggtitle('Price vs Carat')
#График плотности распределения цен в зависимости от качества огранки
ggplot(train)+
geom_density(aes(x=price,fill=cut),alpha=0.2)+
xlab('Price')+
ylab('Density')+
ggtitle('Price distribution')
#График, показывающий распределение цен в зависимости от глубины огранки и качества огранки
ggplot(train)+
geom_point(aes(y=price,x=depth,color=cut))+
xlab('Depth')+
ylab('Price')
# Определение зависимой переменной
response_variable <- "price"
# Обучение модели Random Forest
rf_model = randomForest(formula = as.formula(paste(response_variable, "~ .")), data = train, ntree = 100)
imp=importance(rf_model) # вычисление важности каждого признака в модели
vars=dimnames(imp)[[1]] # извлечение имени признаков
imp=data.frame(vars=vars,imp=as.numeric(imp[,1])) # создание DataFrame с именем признака и его важностью в модели
imp=imp[order(imp$imp,decreasing=T),] #сортировка по убыванию важности признаков
par(mfrow=c(1,2)) #для построения двух графиков
varImpPlot(rf_model,main='Variable Importance Plot: Base Model')
plot(rf_model,main='Error vs No. of trees plot: Base Model')
# Предсказание на тестовом наборе
predictions = predict(rf_model, newdata = test)
rmse1 = sqrt(mean((test$price - predictions)^2))
cat("RMSE: ", rmse1, "\n")
set.seed(123)
selected=c(as.character(imp[1:6,1]),'price') #берём первые 6 факторов
rf_model = randomForest(formula = as.formula(paste(response_variable, "~ .")), data = train[, selected], ntree = 100, mtry = 3, maxdepth = 10, nodesize = 5)
par(mfrow=c(1,2)) #для построения двух графиков
varImpPlot(rf_model,main='Variable Importance Plot: Base Model')
plot(rf_model,main='Error vs No. of trees plot: Base Model')
# Предсказание на тестовом наборе
predictions = predict(rf_model, newdata = test)
rmse2 = sqrt(mean((test$price - predictions)^2))
cat("RMSE: ", rmse2, "\n")
actual=test$price
result=data.frame(actual=actual,predicted=predictions)
ggplot(result)+
geom_point(aes(x=actual,y=predicted,color=predicted-actual),alpha=0.7)+
ggtitle('Plotting Error')
#это связано с тем, что количесво цен до 3000$ намного больше, поэтому модель достаточно хорошо предскажет цены для простых брилиантов
new_data = data.frame(carat = 0.31, color = "J",	clarity = "SI2",	x=4.34, y=4.35,	z=2.75) #22520 строка Лучший результат 10581.99
# Нормализуем новые данные с использованием средних и стандартных отклонений из таблицы diamonds
new_data$carat = (new_data$carat - means["carat"]) / sds["carat"]
new_data$depth = (new_data$depth - means["depth"]) / sds["depth"]
new_data$table = (new_data$table - means["table"]) / sds["table"]
new_data$x = (new_data$x - means["x"]) / sds["x"]
new_data$y = (new_data$y - means["y"]) / sds["y"]
new_data$z = (new_data$z - means["z"]) / sds["z"]
new_data$color = factor(new_data$color, levels = levels(diamonds$color))
new_data$clarity = factor(new_data$clarity, levels = levels(diamonds$clarity))
new_data$cut = factor(new_data$cut, levels = levels(diamonds$cut))
prediction = predict(rf_model, new_data)
prediction
saveRDS(rf_model, file = "RandomForest.rds")
# Создание матриц DMatrix для обучающей и тестовой выборок
train[] <- lapply(train, as.numeric) #приводим значения столбцов обучающей выборки к числовому типу
test[] <- lapply(test, as.numeric) #приводим значения столбцов тестовой выборки к числовому типу
dtrain <- xgb.DMatrix(as.matrix(train[, -1]), label = train$price) #создаем матрицу для обучающей выборки, убираем первый столбец и указываем целевую переменную
dtest <- xgb.DMatrix(as.matrix(test[, -1]), label = test$price) #создаем матрицу для тестовой выборки, убираем первый столбец и указываем целевую переменную
# Определение параметров модели XGBoost
params <- list( #создаем список параметров модели
objective = "reg:squarederror", #целевая переменная для регрессии
eta = 0.01, #скорость обучения (learning rate)
max_depth = 6, #максимальная глубина дерева
min_child_weight = 1, #минимальное количество обучающих примеров в листе дерева
subsample = 0.8, #доля обучающих примеров, используемых для обучения каждого дерева
colsample_bytree = 0.8 #доля признаков, используемых для обучения каждого дерева
)
watchlist <- list(train = dtrain, valid = dtest) #список данных для отслеживания обучения
# Обучение модели на обучающей выборке
model <- xgb.train( #обучаем модель
params = params, #передаем параметры модели
data = dtrain, #передаем данные для обучения
nrounds = 5000, #максимальное количество итераций
early_stopping_rounds = 10, #количество итераций без улучшения модели, при которых обучение останавливается
watchlist = watchlist, #передаем список данных для отслеживания обучения
verbose = 0 #уровень вывода информации об обучении модели
)
saveRDS(model, file = "XGBoost.rds")
predictions = predict(model, newdata = dtest)
rmse3 = sqrt(mean((test$price - predictions)^2))
cat("RMSE: ", rmse3, "\n")
new_data = data.frame(carat = 1.26, cut = "Ideal", color = "G",	clarity = "VVS2",	depth=60.7,	table=56,	x=7.05, y=7.03,	z=4.27) #22520 строка Лучший результат 10581.99
new_data$carat = (new_data$carat - means["carat"]) / sds["carat"]
new_data$depth = (new_data$depth - means["depth"]) / sds["depth"]
new_data$table = (new_data$table - means["table"]) / sds["table"]
new_data$x = (new_data$x - means["x"]) / sds["x"]
new_data$y = (new_data$y - means["y"]) / sds["y"]
new_data$z = (new_data$z - means["z"]) / sds["z"]
new_data$cut <- as.numeric(as.factor(new_data$cut))
new_data$color <- as.numeric(as.factor(new_data$color))
new_data$clarity <- as.numeric(as.factor(new_data$clarity))
dnew <- xgb.DMatrix(as.matrix(new_data))
new_price <- predict(model, dnew)
new_price
colnames(new_data) <- colnames(train)[2:10]
dnew <- xgb.DMatrix(as.matrix(new_data))
new_price <- predict(model, dnew)
new_price
